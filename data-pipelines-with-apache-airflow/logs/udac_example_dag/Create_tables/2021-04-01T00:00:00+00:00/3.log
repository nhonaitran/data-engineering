[2021-04-06 03:04:28,910] {taskinstance.py:851} INFO - Dependencies all met for <TaskInstance: udac_example_dag.Create_tables 2021-04-01T00:00:00+00:00 [queued]>
[2021-04-06 03:04:28,999] {taskinstance.py:851} INFO - Dependencies all met for <TaskInstance: udac_example_dag.Create_tables 2021-04-01T00:00:00+00:00 [queued]>
[2021-04-06 03:04:29,007] {taskinstance.py:1042} INFO - 
--------------------------------------------------------------------------------
[2021-04-06 03:04:29,009] {taskinstance.py:1043} INFO - Starting attempt 3 of 4
[2021-04-06 03:04:29,012] {taskinstance.py:1044} INFO - 
--------------------------------------------------------------------------------
[2021-04-06 03:04:29,122] {taskinstance.py:1063} INFO - Executing <Task(CreateTablesOperator): Create_tables> on 2021-04-01T00:00:00+00:00
[2021-04-06 03:04:29,153] {standard_task_runner.py:52} INFO - Started process 107 to run task
[2021-04-06 03:04:29,262] {standard_task_runner.py:76} INFO - Running: ['airflow', 'tasks', 'run', 'udac_example_dag', 'Create_tables', '2021-04-01T00:00:00+00:00', '--job-id', '30', '--pool', 'default_pool', '--raw', '--subdir', 'DAGS_FOLDER/udac_example_dag.py', '--cfg-path', '/tmp/tmpxm4u9c9i', '--error-file', '/tmp/tmp7n3e6g_m']
[2021-04-06 03:04:29,284] {standard_task_runner.py:77} INFO - Job 30: Subtask Create_tables
[2021-04-06 03:04:29,891] {logging_mixin.py:104} INFO - Running <TaskInstance: udac_example_dag.Create_tables 2021-04-01T00:00:00+00:00 [running]> on host 0ccaa9d2b3f1
[2021-04-06 03:04:30,254] {taskinstance.py:1257} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=udacity
AIRFLOW_CTX_DAG_ID=udac_example_dag
AIRFLOW_CTX_TASK_ID=Create_tables
AIRFLOW_CTX_EXECUTION_DATE=2021-04-01T00:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-04-01T00:00:00+00:00
[2021-04-06 03:04:30,264] {create_tables.py:21} INFO - Creating all tables in schema:
[2021-04-06 03:04:30,403] {base.py:74} INFO - Using connection to: id: redshift. Host: sparkify-redshift.c5c7ovgg2wrr.us-west-2.redshift.amazonaws.com, Port: 5439, Schema: sparkify-song-analytics, Login: data.engineer, Password: XXXXXXXX, extra: None
[2021-04-06 03:04:31,188] {dbapi.py:180} INFO - Running statement: 
        BEGIN;
        DROP TABLE IF EXISTS staging_events;
        CREATE TABLE staging_events (
            artist varchar(256),
            auth varchar(256),
            firstname varchar(256),
            gender varchar(256),
            iteminsession int4,
            lastname varchar(256),
            length numeric(18,0),
            "level" varchar(256),
            location varchar(256),
            "method" varchar(256),
            page varchar(256),
            registration numeric(18,0),
            sessionid int4,
            song varchar(256),
            status int4,
            ts int8,
            useragent varchar(256),
            userid int4
        );
        COMMIT;
    , parameters: None
[2021-04-06 03:04:31,579] {dbapi.py:186} INFO - Rows affected: -1
[2021-04-06 03:04:31,722] {base.py:74} INFO - Using connection to: id: redshift. Host: sparkify-redshift.c5c7ovgg2wrr.us-west-2.redshift.amazonaws.com, Port: 5439, Schema: sparkify-song-analytics, Login: data.engineer, Password: XXXXXXXX, extra: None
[2021-04-06 03:04:32,476] {dbapi.py:180} INFO - Running statement: 
        BEGIN;
        DROP TABLE IF EXISTS staging_songs;
        CREATE TABLE staging_songs (
            num_songs int4,
            artist_id varchar(256),
            artist_name varchar(256),
            artist_latitude numeric(18,0),
            artist_longitude numeric(18,0),
            artist_location varchar(256),
            song_id varchar(256),
            title varchar(256),
            duration numeric(18,0),
            "year" int4
        );
        COMMIT;
    , parameters: None
[2021-04-06 03:04:32,766] {taskinstance.py:1455} ERROR - duplicate key violates unique constraint "pg_class_relname_nsp_index" (possibly caused by concurrent transaction conflict)
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 1112, in _run_raw_task
    self._prepare_and_execute_task_with_callbacks(context, task)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 1285, in _prepare_and_execute_task_with_callbacks
    result = self._execute_task(context, task_copy)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 1315, in _execute_task
    result = task_copy.execute(context=context)
  File "/opt/airflow/plugins/operators/create_tables.py", line 22, in execute
    [redshift.run(create_table) for create_table in SqlQueries.create_table_queries]
  File "/opt/airflow/plugins/operators/create_tables.py", line 22, in <listcomp>
    [redshift.run(create_table) for create_table in SqlQueries.create_table_queries]
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/hooks/dbapi.py", line 184, in run
    cur.execute(sql_statement)
psycopg2.errors.UniqueViolation: duplicate key violates unique constraint "pg_class_relname_nsp_index" (possibly caused by concurrent transaction conflict)

[2021-04-06 03:04:32,778] {taskinstance.py:1503} INFO - Marking task as UP_FOR_RETRY. dag_id=udac_example_dag, task_id=Create_tables, execution_date=20210401T000000, start_date=20210406T030428, end_date=20210406T030432
[2021-04-06 03:04:32,851] {local_task_job.py:146} INFO - Task exited with return code 1
