# Data Lakes with Spark
A music streaming startup, Sparkify, has grown their user base and song
database even more and want to move their data warehouse to a data lake. Their
data resides in S3, in a directory of JSON logs on user activity on the app,
as well as a directory with JSON metadata on the songs in their app.

In this project, we're building an ETL pipeline that extracts their data from
S3, processes them using Spark, and loads the data back into S3 as a set of
dimensional tables. This will allow their analytics team to continue finding
insights in what songs their users are listening to.

## Datasets
The song and user activity datasets now reside in S3. Here are the S3 links
for each:

Song data: s3://udacity-dend/song_data
Log data: s3://udacity-dend/log_data

### Song Dataset
The first dataset is a subset of real data from the Million Song Dataset. Each 
file is in JSON format and contains metadata about a song and the artist of 
that song. The files are partitioned by the first three letters of each song's 
track ID. For example, here are filepaths to two files in this dataset.
```
song_data/A/B/C/TRABCEI128F424C983.json
song_data/A/A/B/TRAABJL12903CDCF1A.json
```
And below is an example of what a single song file, TRAABJL12903CDCF1A.json, 
looks like.

{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}

### Log Dataset
The second dataset consists of log files in JSON format generated by this event 
simulator based on the songs in the dataset above. These simulate app activity 
logs from an imaginary music streaming app based on configuration settings.

The log files in the dataset you'll be working with are partitioned by year and 
month. For example, here are filepaths to two files in this dataset.
```
log_data/2018/11/2018-11-12-events.json
log_data/2018/11/2018-11-13-events.json
```
And below is an example of what the data in a log file, 2018-11-12-events.json, looks like.

![log-data](../assets/log-data.png)

## Star Schema for Song Play Analysis
Using the song and log datasets, a star schema optimized for queries on song play analysis
is created

The database schema includes the `songplays` fact table and four dimension tables 
that are linked by the primary keys defined in the dimension tables.

Users' listening sessions data is stored in the `songplays` fact table.  This 
table only contains the primary key for the song, artist, user, and timestamp 
in each entry.  Metadata such as song title or artist's or user's name, etc. 
can be queried by joining the fact table with the respective dimension tables 
using these primary keys.

![star-schema](../assets/star-schema.png)

Below is brief description of the fact and dimensions tables of the star 
schema created that is optimized for queries on song play analysis.

### Fact Table
1. songplays - records in log data associated with song plays.  This table 
contains the session information, with referential key for specific user, 
song played, and the song's artist linked to the `users`, `songs`, `artists`, 
and `time` tables, respectively.

### Dimension Tables
1. users - users captured from log dataset
2. songs - songs in music database
3. artists - artists in music database
5. time - timestamps of records in `songplays` broken down into specific units

Each dimension table has a primary key which is used to join with the 
`songplays` fact table for getting more detail information about the 
song(s), the artist(s), the user(s), and/or relevant time related to 
the listening ses"monthsion.

## Operations

The `dl.cfg` contains the AWS credentials required to access the AWS S3 buckets
where the songs and log datasets are stored.

The `etl.py` is the ETL python program that reads data from S3, processes that 
data using Spark, and writes the results back to S3.

To kick off the ETL process, issue `run` or `all` Make target from the root 
directory of the repo as follows:
```
$ make run
```

`run` Make target assumes the virtual environment already setup, whereas `all`
Make target will create the virtual environment, install dependencies, and kick 
off the  ETL process to build the star schema and populate it extracted data.

## ETL Process
While it is possible for pyspark to infer the schema from the song and log json
files, I opt to define the schema for the song and log datasets.  This gives me 
better control over how the columns are mapped when the raw data is loaded.

Unlike previous database (relational or NoSQL) technology, data processing with
Spark works data files on disk or data loaded in memory.  Because spark runs in
distributed environment, the data files are loaded in parallel, and thus enables
fast processing of large datasets.
