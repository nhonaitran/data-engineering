{
 "cells": [
  {
   "source": [
    "## Design Cassandra database for Analyzing Songs And User Listening Sessions Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "This notebook includes the ETL pipeline a) to process the event data collected by the music streaming app, and b) create a Cassandra keyspace for the analysis team to query against the data to understand what songs users are listening to.\n",
    "\n",
    "The event data is stored in CSV files, one for each day collected.  Files are placed in `/data/event_data/` directory.\n",
    "\n",
    "The ETL pipeline process traverses the data directory, reads in data from each file and then write out a single CSV file, `/data/processed_events/event_datafile_new.csv` file.\n",
    "\n",
    "Once the new file is created, its content is loaded loaded into the pandas dataframe for analysis.\n",
    "\n",
    "Helper functions `create_table`, `populate_table`, `fetch`, `drop_table`, `pandas_factory` are created aid with data analysis:\n",
    "* `create_table` - given a current session, create a table in the Cassandra keyspace with given table name and columns for the table from `table_columns` arg.  Primary keys for the table is specified with `primary_keys` arg.  The optional `clustering_key` arg is for specifying additional key for primary key and sorting\n",
    "* `populate_table` - populating the table with specific fields loaded from the pandas dataframe\n",
    "* `fetch` - fetch data using giving SQL query statement\n",
    "* `drop_table` - delete table from keyspace as needed\n",
    "* `pandas_factory` - specify panda dataframe as the data structure return query results \n",
    "\n",
    "Querying data is a two-steps task. \n",
    "\n",
    "1. you need to invoke `create_table` to create a table and populate it with data extracted from data frame,\n",
    "2. create the SQL statement you want to run against the table and invoke `fetch` function to get the result of the query."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part I. ETL Pipeline for Pre-Processing the Files"
   ]
  },
  {
   "source": [
    "#### Import Python packages "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cassandra\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating list of filepaths to process original event csv data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/Users/ai.tran/src/data-engineering/data-modeling/etl-pipeline\n"
     ]
    }
   ],
   "source": [
    "# checking current working directory\n",
    "print(os.getcwd())\n",
    "\n",
    "# Get your current folder and subfolder event data\n",
    "filepath = os.getcwd() + '/data/event_data'\n",
    "\n",
    "# Create a for loop to create a list of files and collect each filepath\n",
    "for root, dirs, files in os.walk(filepath):\n",
    "     \n",
    "    # join the file path and roots with the subdirectories using glob\n",
    "    file_path_list = glob.glob(os.path.join(root,'*'))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing the files to create the data file csv that will be used for Apache Casssandra tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/Users/ai.tran/src/data-engineering/data-modeling/etl-pipeline/data/processed_events/event_datafile_new.csv\n"
     ]
    }
   ],
   "source": [
    "# specify the name of the file in which the extracted data is stored\n",
    "processed_events_file = os.getcwd() + '/data/processed_events/event_datafile_new.csv'\n",
    "\n",
    "# initiating an empty list of rows that will be generated from each file\n",
    "full_data_rows_list = [] \n",
    "    \n",
    "# for every filepath in the file path list \n",
    "for f in file_path_list:\n",
    "    # reading csv file \n",
    "    with open(f, 'r', encoding = 'utf8', newline='') as csvfile: \n",
    "        # creating a csv reader object \n",
    "        csvreader = csv.reader(csvfile) \n",
    "        next(csvreader)\n",
    "        \n",
    "        # extracting each data row one by one and append it        \n",
    "        for line in csvreader:\n",
    "            #print(line)\n",
    "            full_data_rows_list.append(line) \n",
    "            \n",
    "# uncomment the code below if you would like to get total number of rows \n",
    "# print(len(full_data_rows_list))\n",
    "# uncomment the code below if you would like to check to see what the list of \n",
    "# event data rows will look like\n",
    "# print(full_data_rows_list)\n",
    "\n",
    "# creating a smaller event data csv file called event_datafile_full csv \n",
    "# that will be used to insert data into the Apache Cassandra tables\n",
    "csv.register_dialect('myDialect', quoting=csv.QUOTE_ALL, skipinitialspace=True)\n",
    "\n",
    "with open(processed_events_file, 'w', encoding = 'utf8', newline='') as f:\n",
    "    writer = csv.writer(f, dialect='myDialect')\n",
    "    writer.writerow(['artist','firstName','gender','itemInSession','lastName','length',\\\n",
    "                'level','location','sessionId','song','userId'])\n",
    "    for row in full_data_rows_list:\n",
    "        if (row[0] == ''):\n",
    "            continue\n",
    "        writer.writerow((row[0], row[2], row[3], row[4], row[5], row[6], row[7], row[8], row[12], row[13], row[16]))\n",
    "\n",
    "print(processed_events_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        artist firstName gender  itemInSession  lastName     length level  \\\n",
       "0     Harmonia      Ryan      M              0     Smith  655.77751  free   \n",
       "1  The Prodigy      Ryan      M              1     Smith  260.07465  free   \n",
       "2        Train      Ryan      M              2     Smith  205.45261  free   \n",
       "3  Sony Wonder    Samuel      M              0  Gonzalez  218.06975  free   \n",
       "4    Van Halen     Tegan      F              2    Levine  289.38404  paid   \n",
       "\n",
       "                               location  sessionId  \\\n",
       "0    San Jose-Sunnyvale-Santa Clara, CA        583   \n",
       "1    San Jose-Sunnyvale-Santa Clara, CA        583   \n",
       "2    San Jose-Sunnyvale-Santa Clara, CA        583   \n",
       "3  Houston-The Woodlands-Sugar Land, TX        597   \n",
       "4           Portland-South Portland, ME        602   \n",
       "\n",
       "                                             song  userId  \n",
       "0                                   Sehr kosmisch      26  \n",
       "1                                 The Big Gundown      26  \n",
       "2                                        Marry Me      26  \n",
       "3                                       Blackbird      61  \n",
       "4  Best Of Both Worlds (Remastered Album Version)      80  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>artist</th>\n      <th>firstName</th>\n      <th>gender</th>\n      <th>itemInSession</th>\n      <th>lastName</th>\n      <th>length</th>\n      <th>level</th>\n      <th>location</th>\n      <th>sessionId</th>\n      <th>song</th>\n      <th>userId</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Harmonia</td>\n      <td>Ryan</td>\n      <td>M</td>\n      <td>0</td>\n      <td>Smith</td>\n      <td>655.77751</td>\n      <td>free</td>\n      <td>San Jose-Sunnyvale-Santa Clara, CA</td>\n      <td>583</td>\n      <td>Sehr kosmisch</td>\n      <td>26</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>The Prodigy</td>\n      <td>Ryan</td>\n      <td>M</td>\n      <td>1</td>\n      <td>Smith</td>\n      <td>260.07465</td>\n      <td>free</td>\n      <td>San Jose-Sunnyvale-Santa Clara, CA</td>\n      <td>583</td>\n      <td>The Big Gundown</td>\n      <td>26</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Train</td>\n      <td>Ryan</td>\n      <td>M</td>\n      <td>2</td>\n      <td>Smith</td>\n      <td>205.45261</td>\n      <td>free</td>\n      <td>San Jose-Sunnyvale-Santa Clara, CA</td>\n      <td>583</td>\n      <td>Marry Me</td>\n      <td>26</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Sony Wonder</td>\n      <td>Samuel</td>\n      <td>M</td>\n      <td>0</td>\n      <td>Gonzalez</td>\n      <td>218.06975</td>\n      <td>free</td>\n      <td>Houston-The Woodlands-Sugar Land, TX</td>\n      <td>597</td>\n      <td>Blackbird</td>\n      <td>61</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Van Halen</td>\n      <td>Tegan</td>\n      <td>F</td>\n      <td>2</td>\n      <td>Levine</td>\n      <td>289.38404</td>\n      <td>paid</td>\n      <td>Portland-South Portland, ME</td>\n      <td>602</td>\n      <td>Best Of Both Worlds (Remastered Album Version)</td>\n      <td>80</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "df = pd.read_csv(processed_events_file, header=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(6820, 11)"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part II. Load processed data file and create tables for analyzing songs and users' listening sessions information. \n",
    "\n",
    "#### The event_datafile_new.csv contains the following columns: \n",
    "- artist \n",
    "- firstName of user\n",
    "- gender of user\n",
    "- item number in session\n",
    "- last name of user\n",
    "- length of the song\n",
    "- level (paid or free song)\n",
    "- location of the user\n",
    "- sessionId\n",
    "- song title\n",
    "- userId\n",
    "\n",
    "The image below is a screenshot of what the denormalized data should appear like in the <font color=red>**event_datafile_new.csv**</font> after the code above is run:<br>\n",
    "\n",
    "<img src=\"assets/image_event_datafile_new.jpg\">"
   ]
  },
  {
   "source": [
    "#### Define functions for creating table, populating table with datasource, run query and return results as panda dataframe, and dropping table.  The pandas factory is defined to be used as the row factory for the session"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table(session, table_name, table_columns, primary_keys, clustering_keys=None):\n",
    "    \"\"\" \n",
    "        This function create a table and populate it with data from a global \n",
    "        defined data source.\n",
    "\n",
    "        A table with `table_name` is created in given keyspace with columns defined \n",
    "        from given `table_columns` list.\n",
    "        \n",
    "        The primary keys for table are specified in `primary_keys` and clustering_keys.\n",
    "        \n",
    "        This includes running `drop table` operation first to ensure existing table \n",
    "        is deleted prior to creating a new one.\n",
    "    \"\"\"\n",
    "\n",
    "    if session is None or table_name is None or table_columns is None or primary_keys is None or len(primary_keys) == 0:\n",
    "        print(\"Must provide active session, table name and columns plus data type, and one or more primary keys\")\n",
    "        return\n",
    "\n",
    "    # drop existing table with same name\n",
    "    drop_table(session, table_name)\n",
    "\n",
    "    # declare table\n",
    "    create_stmt = f\"CREATE TABLE IF NOT EXISTS {table_name} (\"\n",
    "\n",
    "    # define columns in table with their corresponding data type\n",
    "    for column, data_type in table_columns.items():\n",
    "        create_stmt = create_stmt + f\"{column} {data_type}, \"\n",
    "\n",
    "    # define primary keys for table\n",
    "    create_stmt = create_stmt + f\"PRIMARY KEY (({', '.join(primary_keys)})\"\n",
    "\n",
    "    # append clustering keys for sorting if given\n",
    "    if clustering_keys is not None:\n",
    "        create_stmt = create_stmt + f\", {', '.join(clustering_keys)}\"\n",
    "    \n",
    "    create_stmt = create_stmt + \"))\"\n",
    "    print(create_stmt)  \n",
    "\n",
    "    # create operation\n",
    "    try:\n",
    "        session.execute(create_stmt)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    # populate the table with data\n",
    "    populate_table(session, table_name, list(table_columns.keys()))\n",
    "\n",
    "\n",
    "def populate_table(session, table_name, table_columns):\n",
    "    \"\"\" \n",
    "        Populate the table with data extracted from global data source, df. \n",
    "        \n",
    "        Notes: The global data source is used for convenient, without the need \n",
    "        to open the same csv file and create the dataframe each time. \n",
    "        \n",
    "        If the data file is different for each table, it is recommended that the \n",
    "        the data file should be provided and data be loaded into new dataframe\n",
    "        each time function is invoked.\n",
    "    \"\"\"\n",
    "\n",
    "    if session is None or table_name is None or table_columns is None:\n",
    "        print(\"Must provide active session, table name plus columns plus data type\")\n",
    "        return\n",
    "\n",
    "    insert_stmt = f\"INSERT INTO {table_name} ({', '.join(table_columns)}) VALUES ({'%s, '*(len(table_columns)-1)}%s)\"\n",
    "    try:\n",
    "        [session.execute(insert_stmt, tuple(row)) for row in df[table_columns].to_numpy()]\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "\n",
    "def fetch(session, query):\n",
    "    \"\"\" Fetch data from database using given query statement \"\"\"\n",
    "\n",
    "    if session is None or query is None:\n",
    "        print(\"Must provide active session, and query to execute\")\n",
    "        return\n",
    "\n",
    "    print(f\"===== Query: =======\\n{query}\\n\")\n",
    "    try:\n",
    "        result = session.execute(query, timeout=None)\n",
    "        dataframe = result._current_rows\n",
    "        return dataframe\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    \n",
    "\n",
    "def drop_table(session, table_name):\n",
    "    \"\"\" Drop the given table from keyspace of given session \"\"\"\n",
    "\n",
    "    drop_stmt = f\"DROP TABLE IF EXISTS {table_name}\"\n",
    "    try:\n",
    "        session.execute(drop_stmt)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "\n",
    "def pandas_factory(colnames, rows):\n",
    "    \"\"\" Define pandas dataframe as the row factory for cassandra session \"\"\"\n",
    "    return pd.DataFrame(rows, columns=colnames)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should make a connection to a single-node Cassandra cluster instance on local machine \n",
    "# (127.0.0.1), at port 6000.  Note that the default port for cassandra database is 9042\n",
    "# \n",
    "try:\n",
    "    from cassandra.cluster import Cluster\n",
    "    cluster = Cluster(['127.0.0.1'], 6000)\n",
    "\n",
    "    # To establish connection and begin executing queries, need a session\n",
    "    session = cluster.connect()\n",
    "    session.row_factory = pandas_factory\n",
    "    session.default_fetch_size = None\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Keyspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    session.execute(\"\"\"\n",
    "        CREATE KEYSPACE IF NOT EXISTS udacity\n",
    "        WITH REPLICATION = {'class':'SimpleStrategy', 'replication_factor':1}\n",
    "    \"\"\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Keyspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    session.set_keyspace('udacity')\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we need to create tables to run the following queries. Remember, with Apache Cassandra you model the database tables on the queries you want to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create queries to ask the following three questions of the data\n",
    "\n",
    "#### Query 1: Give me the artist, song title and song's length in the music app history that was heard during  sessionId = 338, and itemInSession  = 4\n",
    "\n",
    "In order to run this query, the table must contain the following fields:\n",
    "* sessionId:        int\n",
    "* itemInSession:    int\n",
    "* artist:           text\n",
    "* song:             text\n",
    "* length:           float\n",
    "\n",
    "These fields are defined in the `table_columns` argument.\n",
    "\n",
    "The primary key is a composite key comprised of columns that are used for querying: sessionId and itemInSession.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CREATE TABLE IF NOT EXISTS song_history_by_session (sessionId int, itemInSession int, artist text, song text, length float, PRIMARY KEY ((sessionId, itemInSession)))\n",
      "===== Query: =======\n",
      "\n",
      "    SELECT artist, song, length\n",
      "    FROM song_history_by_session\n",
      "    WHERE sessionId=338 AND itemInSession=4\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      artist                             song      length\n",
       "0  Faithless  Music Matters (Mark Knight Dub)  495.307312"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>artist</th>\n      <th>song</th>\n      <th>length</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Faithless</td>\n      <td>Music Matters (Mark Knight Dub)</td>\n      <td>495.307312</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "table_1 = \"song_history_by_session\"\n",
    "\n",
    "## create a table for storing data\n",
    "create_table(session, \n",
    "    table_name=table_1, \n",
    "    table_columns={'sessionId':'int', 'itemInSession':'int', 'artist':'text', 'song':'text', 'length':'float'}, \n",
    "    primary_keys=('sessionId', 'itemInSession')\n",
    ")\n",
    "\n",
    "## Add in the SELECT statement to verify the data was entered into the table\n",
    "query = f\"\"\"\n",
    "    SELECT artist, song, length\n",
    "    FROM {table_1}\n",
    "    WHERE sessionId=338 AND itemInSession=4\"\"\"\n",
    "df1 = fetch(session, query)\n",
    "df1"
   ]
  },
  {
   "source": [
    "#### Query 2: Give me only the following: name of artist, song (sorted by itemInSession) and user (first and last name) for userid = 10, sessionid = 182\n",
    "\n",
    "In order to run this query, the table must contain the following fields:\n",
    "* userId: int\n",
    "* sessionId: int\n",
    "* artist: text\n",
    "* song: text\n",
    "* firstName: text\n",
    "* lastName: text\n",
    "\n",
    "These fields are defined in the `table_columns` argument.\n",
    "\n",
    "The primary key of this table is a composite key including `userId`, `sessionId`, and the clustering key `itemInSession`. \n",
    "\n",
    "`itemInSession` is added as clustering column for sorting the resultset."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CREATE TABLE IF NOT EXISTS song_history_by_user_session (sessionId int, userId int, itemInSession int, artist text, song text, firstName text, lastName text, PRIMARY KEY ((sessionId, userId), itemInSession))\n",
      "===== Query: =======\n",
      "\n",
      "    SELECT artist, song, firstName, lastName\n",
      "    FROM song_history_by_user_session\n",
      "    WHERE userId=10 AND sessionId=182 \n",
      "    ORDER BY itemInSession\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "              artist                                               song  \\\n",
       "0   Down To The Bone                                 Keep On Keepin' On   \n",
       "1       Three Drives                                        Greece 2000   \n",
       "2  Sebastien Tellier                                          Kilometer   \n",
       "3      Lonnie Gordon  Catch You Baby (Steve Pitron & Max Sanna Radio...   \n",
       "\n",
       "  firstname lastname  \n",
       "0    Sylvie     Cruz  \n",
       "1    Sylvie     Cruz  \n",
       "2    Sylvie     Cruz  \n",
       "3    Sylvie     Cruz  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>artist</th>\n      <th>song</th>\n      <th>firstname</th>\n      <th>lastname</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Down To The Bone</td>\n      <td>Keep On Keepin' On</td>\n      <td>Sylvie</td>\n      <td>Cruz</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Three Drives</td>\n      <td>Greece 2000</td>\n      <td>Sylvie</td>\n      <td>Cruz</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Sebastien Tellier</td>\n      <td>Kilometer</td>\n      <td>Sylvie</td>\n      <td>Cruz</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Lonnie Gordon</td>\n      <td>Catch You Baby (Steve Pitron &amp; Max Sanna Radio...</td>\n      <td>Sylvie</td>\n      <td>Cruz</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "table_2 = \"song_history_by_user_session\"\n",
    "\n",
    "create_table(session, \n",
    "    table_name=table_2, \n",
    "    table_columns={'sessionId':'int', 'userId':'int', 'itemInSession':'int', 'artist':'text', 'song':'text', 'firstName':'text', 'lastName':'text'}, \n",
    "    primary_keys=['sessionId', 'userId'],\n",
    "    clustering_keys=['itemInSession']\n",
    ")\n",
    "\n",
    "query = f\"\"\"\n",
    "    SELECT artist, song, firstName, lastName\n",
    "    FROM {table_2}\n",
    "    WHERE userId=10 AND sessionId=182 \n",
    "    ORDER BY itemInSession\"\"\"\n",
    "df2 = fetch(session, query)\n",
    "df2"
   ]
  },
  {
   "source": [
    "#### Query 3: Give me every user name (first and last) in my music app history who listened to the song 'All Hands Against His Own'\n",
    "\n",
    "In order to run this query, the table must contain the following fields:\n",
    "* song: text\n",
    "* userId: int\n",
    "* firstName: text\n",
    "* lastName: text\n",
    "\n",
    "These fields are defined in the table_columns argument.\n",
    "\n",
    "The primary key (PK) for this table is a composite key comprised of `song` and `userId` columns.  \n",
    "`song` is used as PK because it is used for looking up the specific song.  `userId` column is added as the second key\n",
    "to ensure uniqueness of each row in the resultset.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CREATE TABLE IF NOT EXISTS song_listened_by_users (song text, userId int, firstName text, lastName text, PRIMARY KEY ((song), userId))\n",
      "===== Query: =======\n",
      "\n",
      "    SELECT firstName, lastName \n",
      "    FROM song_listened_by_users \n",
      "    WHERE song='All Hands Against His Own'\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "    firstname lastname\n",
       "0  Jacqueline    Lynch\n",
       "1       Tegan   Levine\n",
       "2        Sara  Johnson"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>firstname</th>\n      <th>lastname</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Jacqueline</td>\n      <td>Lynch</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Tegan</td>\n      <td>Levine</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Sara</td>\n      <td>Johnson</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "table_3 = \"song_listened_by_users\"\n",
    "\n",
    "create_table(session, \n",
    "    table_name=table_3, \n",
    "    table_columns={'song':'text', 'userId':'int', 'firstName':'text', 'lastName':'text'}, \n",
    "    primary_keys=['song'],\n",
    "    clustering_keys=['userId']\n",
    ")\n",
    "\n",
    "query = f\"\"\"\n",
    "    SELECT firstName, lastName \n",
    "    FROM {table_3} \n",
    "    WHERE song='All Hands Against His Own'\"\"\"\n",
    "df3 = fetch(session, query)\n",
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop the tables before closing out the sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for table in [table_1, table_2, table_3]:\n",
    "    drop_table(session, table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Close the session and cluster connectionÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.shutdown()\n",
    "cluster.shutdown()"
   ]
  },
  {
   "source": [
    "TODO:\n",
    "performance improvement"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}