# Sparkify AWS Data Warehouse for Song Play Analysis

This repo contains python modules and utility shell scripts that automate the 
ETL workflow pipeline for transfering music song and log datasets from json 
files into a PostgreSQL database for song play analysis.

The json data files come in two datasets:

### Song Dataset
The song dataset is a subset of real data from the [Million Song Dataset](http://millionsongdataset.com/). 
Each file is in JSON format and contains metadata about a song and the artist 
of that song. The files are partitioned by the first three letters of each song's 
track ID. For example, here are filepaths to two files in this dataset.

```
song_data/A/B/C/TRABCEI128F424C983.json
song_data/A/A/B/TRAABJL12903CDCF1A.json
```
And below is an example of what a single song file, TRAABJL12903CDCF1A.json, looks like.

```
{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}
```

### Log Dataset
The second database consists of log files in JSON format generated by this 
[event simulator](https://github.com/Interana/eventsim) based on the songs 
in the dataset above. These simulate activity logs from a music streaming app 
based on specified configurations.

The log files in the dataset are partitioned by year and month. For example, 
here are filepaths to two files in this dataset.

```
log_data/2018/11/2018-11-12-events.json
log_data/2018/11/2018-11-13-events.json
```

And below is an example of what the data in a log file, 2018-11-12-events.json, looks like.

![log-data](assets/log-data.png)

The ETL python module or the bulk-import script can be used to import the data 
contained in these json files into a Postgresql database, which can be used for queries to analysis song play


## Star Schema for Sparkify AWS Data Warehouse

To facility song play analysis, the music PostgreSQL database based on a star 
schema is created to store the imported song and log data.

The database schema includes the `songplays` fact table and four dimension tables 
that are linked by the primary keys defined in the dimenion tables.

Users' listening sessions data is stored in the `songplays` fact table.  This 
table only contains the primary key for the song, artist, user, and timestamp 
in each entry.  Metadata such as song title or artist's or user's name, etc. 
can be queried by joining the fact table with the respective dimension tables 
using these primary keys.

![star-schema](assets/star-schema.png)

Below is brief description of the fact and dimentions tables of the star 
schema created that is optimized for queries on song play analysis.

### Fact Table
1. songplays - records in log data associated with song plays.  This table 
contains the session information, with referential key for specific user, 
song played, and the song's artist linked to the `users`, `songs`, `artists`, 
and `time` tables, respectively.

### Dimension Tables
1. users - users captured from log dataset
2. songs - songs in music database
3. artists - artists in music database
5. time - timestamps of records in `songplays` broken down into specific units

Each dimension table has a primary key which is used to join with the 
`songplays` fact table for getting more detail information about the 
song(s), the artist(s), the user(s), and/or relevant time related to 
the listening session.

## ETL Pipeline Workflow

The data warehouse uses the AWS Redshift columnar database to 
store event logs data extracted from the user listening sessions and songs information
to support user listening analysis.

Session listening log data are stored in the `log-data` json files.  The data
is loaded into the `staging_events` table, which is used for populating the
`songplays` fact table.

Songs' and users' data are stored in `song-data` json files.  The data
are loaded and stored in the `staging_songs` table for latter 
extracted to the dimension tables.

The ETL pipeline for launching a data warehouse with relevant facts and dimensions
tables to support songs analysis can be executed with the following
command options.

Launching a new cluster uses the infrastructure-as-code `create_table.py`
module to create a new cluster in AWS with provided configurations
provided in the `dwh.cfg` file.

Once the cluster is up and running, the song-analytics star schema
can be generated by running the `--command create` option of
the `create_tables.py` module.

This command option drops all existing tables in the schema
and recreate the staging tables as well as the facts and
dimension tables.

Checking current status of the cluster can be done easier by issuing
the `--command status` option.

Use the `--command delete` option to clean up all AWS resources after work is done,

The `etl.py` module loads logs and songs data currently stored in a S3 bucket 
into the `staging_events` and `staging_songs` tables.  It also perform ETL
process to then load data into following fact and dimensions tables:
* `songplays` fact
* `users` dimension
* `songs` dimension
* `artists` dimension
* `time` dimension

### CLI Operations

Below is the summary of all operations to launch the data warehose in AWS and 
relevant ETL commands to populate the tables in it.

To launch a new cluster:
```
$ python create_tables.py --command launch 
```

To delete an existing cluster:
```
$ python create_tables.py --command delete 
```

To create the database schema with all tables:
```
$ python create_tables.py --command create
```

To check current status of the cluster:
```
$ python create_tables.py --command status
```

To run ETL process to load data into staging
tables and populate relevant tables from staging tables:
```
$ python etl.py
```


## Sample Queries

Here are some of queries that you can run to view the import data.

Login to the database:
```
$ psql -h 127.0.0.1 -p 5432 -U student -d sparkifydb
```

General data about the datasets:
```
sparkifydb=> select count(*) from users;
 count
-------
    96
(1 row)

sparkifydb=> select count(*) from artists;
 count
-------
    69
(1 row)

sparkifydb=> select count(*) from songs;
 count
-------
    71
(1 row)

```

Examine the number of songs played with free vs paid level per user.
```
sparkifydb=> select user_id, level, count(*)
from songplays
group by user_id, level
order by user_id;
```
The above query shows users tend to listen to more songs with `paid` level compared to their `free` level, which is expected since there might be a limit of free songs they can listen to.

Average length of songs played:
```
sparkifydb=> select avg(s.duration) from songplays sp left join songs s on sp.song_id = s.song_id group by s.title order by count(*) desc limit 10;
        avg
-------------------

 269.5832214355469
(2 rows)
```

Top 10 popular artist:
```
sparkifydb=> select a.name, count(*) from songplays sp left join artists a on sp.artist_id = a.artist_id group by a.name order by count(*) desc limit 10;
 name  | count
-------+-------
       |  6819
 Elena |     1
(2 rows)
```

Top 10 popular songs:

```
sparkifydb=> select s.title, count(*) from songplays sp left join songs s on sp.song_id = s.song_id group by s.title order by count(*) desc limit 10;
sparkifydb=> select s.title, count(*) from songplays sp left join songs s on sp.song_id = s.song_id group by s.title order by count(*) desc limit 10;
     title      | count
----------------+-------
                |  6819
 Setanta matins |     1
(2 rows)
```



