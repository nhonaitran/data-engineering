{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "israeli-clinton",
   "metadata": {},
   "source": [
    "Data Engineering Process for Building User Listening Analytics Data Store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confident-coast",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "\n",
    "The data used for this study are is repo contains python modules and utility shell scripts that automate the \n",
    "ETL workflow pipeline for transfering music song and log datasets from json \n",
    "files into a PostgreSQL database for song play analysis.\n",
    "\n",
    "The json data files come in two datasets:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quality-valentine",
   "metadata": {},
   "source": [
    "## Songs Dataset\n",
    "The song dataset is a subset of real data from the [Million Song Dataset](http://millionsongdataset.com/). \n",
    "Each file is in JSON format and contains metadata about a song and the artist \n",
    "of that song. The files are partitioned by the first three letters of each song's \n",
    "track ID. For example, here are filepaths to two files in this dataset.\n",
    "\n",
    "```\n",
    "song_data/A/B/C/TRABCEI128F424C983.json\n",
    "song_data/A/A/B/TRAABJL12903CDCF1A.json\n",
    "```\n",
    "And below is an example of what a single song file, TRAABJL12903CDCF1A.json, looks like.\n",
    "\n",
    "```\n",
    "{\"num_songs\": 1, \"artist_id\": \"ARJIE2Y1187B994AB7\", \"artist_latitude\": null, \"artist_longitude\": null, \"artist_location\": \"\", \"artist_name\": \"Line Renaud\", \"song_id\": \"SOUPIRU12A6D4FA1E1\", \"title\": \"Der Kleine Dompfaff\", \"duration\": 152.92036, \"year\": 0}\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advance-polish",
   "metadata": {},
   "source": [
    "## Log-data/Events Dataset\n",
    "The second database consists of log files in JSON format generated by this \n",
    "[event simulator](https://github.com/Interana/eventsim) based on the songs \n",
    "in the dataset above. These simulate activity logs from a music streaming app \n",
    "based on specified configurations.\n",
    "\n",
    "The log files in the dataset are partitioned by year and month. For example, \n",
    "here are filepaths to two files in this dataset.\n",
    "\n",
    "```\n",
    "log_data/2018/11/2018-11-12-events.json\n",
    "log_data/2018/11/2018-11-13-events.json\n",
    "```\n",
    "\n",
    "And below is an example of what the data in a log file, 2018-11-12-events.json, looks like.\n",
    "\n",
    "![assets](assets/log-data.png)\n",
    "\n",
    "The ETL python module or the bulk-import script can be used to import the data \n",
    "contained in these json files into a Postgresql database, which can be used for queries to analysis song play"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "israeli-wisdom",
   "metadata": {},
   "source": [
    "# UsersListeningAnalytics Database Design\n",
    "\n",
    "To facility song play analysis, the music PostgreSQL database based on a star \n",
    "schema is created to store the imported song and log data.\n",
    "\n",
    "The database schema includes the `songplays` fact table and four dimension tables \n",
    "that are linked by the primary keys defined in the dimenion tables.\n",
    "\n",
    "Users' listening sessions data is stored in the `songplays` fact table.  This \n",
    "table only contains the primary key for the song, artist, user, and timestamp \n",
    "in each entry.  Metadata such as song title or artist's or user's name, etc. \n",
    "can be queried by joining the fact table with the respective dimension tables \n",
    "using these primary keys.\n",
    "\n",
    "![assets](assets/star-schema.png)\n",
    "\n",
    "Below is brief description of the fact and dimentions tables of the star \n",
    "schema created that is optimized for queries on song play analysis.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impressive-stephen",
   "metadata": {},
   "source": [
    "## Fact Table\n",
    "1. songplays - records in log data associated with song plays.  This table \n",
    "contains the session information, with referential key for specific user, \n",
    "song played, and the song's artist linked to the `users`, `songs`, `artists`, \n",
    "and `time` tables, respectively.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "about-zoning",
   "metadata": {},
   "source": [
    "## Dimension Tables\n",
    "1. users - users captured from log dataset\n",
    "2. songs - songs in music database\n",
    "3. artists - artists in music database\n",
    "5. time - timestamps of records in `songplays` broken down into specific units\n",
    "\n",
    "Each dimension table has a primary key which is used to join with the \n",
    "`songplays` fact table for getting more detail information about the \n",
    "song(s), the artist(s), the user(s), and/or relevant time related to \n",
    "the listening session."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patent-heating",
   "metadata": {},
   "source": [
    "# ETL Pipeline Workflow\n",
    "\n",
    "\n",
    "The data ETL workflow uses different technique to import the data dependent \n",
    "on the size of the datasets.  Each technique is described below.\n",
    "\n",
    "## Sample Queries\n",
    "\n",
    "Here are some of queries that you can run to view the import data.\n",
    "\n",
    "Login to the database:\n",
    "```\n",
    "$ psql -h 127.0.0.1 -p 5432 -U student -d sparkifydb\n",
    "```\n",
    "\n",
    "General data about the datasets:\n",
    "```\n",
    "sparkifydb=> select count(*) from users;\n",
    " count\n",
    "-------\n",
    "    96\n",
    "(1 row)\n",
    "\n",
    "sparkifydb=> select count(*) from artists;\n",
    " count\n",
    "-------\n",
    "    69\n",
    "(1 row)\n",
    "\n",
    "sparkifydb=> select count(*) from songs;\n",
    " count\n",
    "-------\n",
    "    71\n",
    "(1 row)\n",
    "\n",
    "```\n",
    "\n",
    "Examine the number of songs played with free vs paid level per user.\n",
    "```\n",
    "sparkifydb=> select user_id, level, count(*)\n",
    "from songplays\n",
    "group by user_id, level\n",
    "order by user_id;\n",
    "```\n",
    "The above query shows users tend to listen to more songs with `paid` level compared to their `free` level, which is expected since there might be a limit of free songs they can listen to.\n",
    "\n",
    "Average length of songs played:\n",
    "```\n",
    "sparkifydb=> select avg(s.duration) from songplays sp left join songs s on sp.song_id = s.song_id group by s.title order by count(*) desc limit 10;\n",
    "        avg\n",
    "-------------------\n",
    "\n",
    " 269.5832214355469\n",
    "(2 rows)\n",
    "```\n",
    "\n",
    "Top 10 popular artist:\n",
    "```\n",
    "sparkifydb=> select a.name, count(*) from songplays sp left join artists a on sp.artist_id = a.artist_id group by a.name order by count(*) desc limit 10;\n",
    " name  | count\n",
    "-------+-------\n",
    "       |  6819\n",
    " Elena |     1\n",
    "(2 rows)\n",
    "```\n",
    "\n",
    "Top 10 popular songs:\n",
    "\n",
    "```\n",
    "sparkifydb=> select s.title, count(*) from songplays sp left join songs s on sp.song_id = s.song_id group by s.title order by count(*) desc limit 10;\n",
    "sparkifydb=> select s.title, count(*) from songplays sp left join songs s on sp.song_id = s.song_id group by s.title order by count(*) desc limit 10;\n",
    "     title      | count\n",
    "----------------+-------\n",
    "                |  6819\n",
    " Setanta matins |     1\n",
    "(2 rows)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brilliant-bennett",
   "metadata": {},
   "source": [
    "## Working with small datasets\n",
    "A dockerized environment with postgresql database is setup to load music data \n",
    "into an independent  music database with ease.  A Makefile is provided in \n",
    "the `docker` directory that can use to either run individual commands as needed \n",
    "or `make all` to perform the entire ETL pipeline.\n",
    "\n",
    "This method is suited for importing small datasets into the database. This \n",
    "is due to the fact that the ETL python module is written to traverse the log \n",
    "and song directories and process the json file by file, which can take a few minutes.\n",
    "\n",
    "Please refer to the `Working with large datasets` section for faster \n",
    "method of importing data using COPY command avaiable in Postgresql.\n",
    "\n",
    "\n",
    "To run the entire ETL pipeline, at the project root directory `etl-pipeline`, \n",
    "type the followings:\n",
    "\n",
    "Download this repo to local machine, and go to the `data-modeling`\n",
    "```\n",
    "$ git clone https://github.com/nhonaitran/data-engineering.git\n",
    "$ cd data-engineering/data-modeling\n",
    "```\n",
    "\n",
    "then project dir, setup virtual environment, install library dependencies, and off we go:\n",
    "```\n",
    "$ cd etl-pipeline\n",
    "$ python -v venv .venv\n",
    "$ source .venv/bin/activate\n",
    "$ pip install -r requirements.txt\n",
    "$ cd docker\n",
    "$ make all\n",
    "```\n",
    "\n",
    "The `all` make target performs the following tasks:\n",
    "1. shut down the docker posrgresl (and cassandra) container, and remove it\n",
    "2. start up a new postgresql container\n",
    "3. create the student role (with appropriate privileges granted) and default database\n",
    "4. run the `create_tables` python module to create the tables in the sparkifydb database\n",
    "5. run the `etl` module with given path where the json files reside\n",
    "6. run sql scripts to verify data is imported into the table as expected.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "usual-atlanta",
   "metadata": {},
   "source": [
    "## Working with large datasets\n",
    "For larger datasets, Postgresql database provides the COPY command that\n",
    "can be used for fast batch importing data into the database.\n",
    "\n",
    "A couple of shell scripts are provided for kick off the data import and \n",
    "ETL processing with SQL scripts. At the project root directory \n",
    "`etl-pipeline`, type the followings:\n",
    "```\n",
    "$ cd etl-pipeline/bin\n",
    "$ sh bulk-import.sh ../data/log_data log_data\n",
    "$ sh bulk-import.sh ../data/song_data song_data\n",
    "$ sh extract_and_verify.sh \n",
    "```\n",
    "\n",
    "The `bulk-import` shell script reads in the content of the json files and \n",
    "invoke the COPY command provided by postgresl to import the json content \n",
    "the given table name\n",
    "\n",
    "The `extract_and_verify` shell script then run ETL SQL scripts to extract \n",
    "songs, artists, users and songplays data from the holding table into their \n",
    "own tables as described in above star schema for the database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thrown-frederick",
   "metadata": {},
   "source": [
    "## ETL Pipeline with Postgresql Database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advance-specification",
   "metadata": {},
   "source": [
    "## ETL Pipeline with Apache Cassandra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pacific-alexander",
   "metadata": {},
   "source": [
    "## ETL Pipeline with AWS Redshift and S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arctic-rescue",
   "metadata": {},
   "source": [
    "## ETL Pipeline with Apache PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chief-central",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
