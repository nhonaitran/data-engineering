{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading and Writing Data with Spark\n",
    "\n",
    "This notebook contains the code from the previous screencast. The only difference is that instead of reading in a dataset from a remote cluster, the data set is read in from a local file. You can see the file by clicking on the \"jupyter\" icon and opening the folder titled \"data\".\n",
    "\n",
    "Run the code cell to see how everything works. \n",
    "\n",
    "First let's import SparkConf and SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're using Spark locally we already have both a sparkcontext and a sparksession running. We can update some of the parameters, such our application's name. Let's just call it \"Our first Python Spark SQL example\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Our first Python Spark SQL example\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if the change went through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.app.name', 'Our first Python Spark SQL example'),\n",
       " ('spark.driver.port', '62533'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.app.id', 'local-1612160490053'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.driver.host', '192.168.146.183')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.146.183:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Our first Python Spark SQL example</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1186ee050>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the app name is exactly how we set it\n",
    "\n",
    "Let's create our first dataframe from a fairly small sample data set. Througout the course we'll work with a log file data set that describes user interactions with a music streaming service. The records describe events such as logging in to the site, visiting a page, listening to the next song, seeing an ad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Users/ai.tran/src/pyviz/flights-viz/data/24.tsv\"\n",
    "daystream = spark.read.csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "daystream.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window as W\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "daystream2 = daystream.withColumn(\"line_num\", F.monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0='_hc\\t1611446400\\t_hs\\t0\\t_c\\t1611446400\\t_s\\t0\\ttype\\tposition\\tident\\tAWQ320\\tchildID\\t197\\t_t\\tb\\tadsb_version\\t0\\tairGround_src\\tA\\tairground\\tA\\talt\\t367\\talt_ft\\t36700\\talt_gnss_src\\tA\\talt_pressure\\t36700\\talt_src\\tA\\tbitmask\\t0\\tclock\\t1611446391\\tcombid\\t1611446400-7\\tdest\\tWMKK\\tfacility\\tfAT-b68f52fd-d1f7-4043-94d4-beb0526d16e5\\tfeed\\tADEPT22\\tfeed_c\\t1611446399\\tfeed_s\\t4180\\tflightlevel\\t367\\tflightlevel_pressure\\t367\\tfp\\tAWQ320-1611267240-schedule-0289:0\\tgSource\\tfeed\\tgs\\t501\\tgs_src\\tA\\thSource\\tfeed\\theading\\t296\\theading_magnetic\\t297.1\\theading_src\\tA\\thexid\\t8A02CB\\tlat\\t1.83881\\tlon\\t103.30525\\tmach\\t0.760\\tnac_p\\t7\\tnac_v\\t0\\tnav_alt\\t30000\\tnav_qnh\\t1009.0\\torig\\tWARR\\totherPedigrees\\t{aireon sita_ups wide} {sita_ups wide} wide\\tpedigree\\taireon wide\\tpos_nic\\t7\\tpos_rc\\t371\\tposition_src\\tA\\tpreferred\\t1\\tpressure\\t220\\tprovenance\\twide\\trecvd\\t1611446400\\treg\\tPKAXU\\troll\\t0.0\\tscore\\t1000\\tsil\\t2\\tsil_type\\tunknown\\tspeed_ias\\t248\\tspeed_tas\\t442\\tsquawk\\t7004\\tsquawk_src\\tU\\ttemperature\\t-50\\ttemperature_quality\\t1\\ttrack_rate\\t-0.03\\tupdateType\\tA\\tvertRate\\t-2592\\tvertRate_geom\\t-2752\\tvertRate_geom_src\\tA\\tvertRate_src\\tU\\twind_dir\\t107\\twind_quality\\t1\\twind_speed\\t59', line_num=0)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daystream2.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|                 _c0|line_num|\n",
      "+--------------------+--------+\n",
      "|_hc\t1611446400\t_h...|       0|\n",
      "|_hc\t1611446400\t_h...|       1|\n",
      "|_hc\t1611446400\t_h...|       2|\n",
      "|_hc\t1611446400\t_h...|       3|\n",
      "|_hc\t1611446400\t_h...|       4|\n",
      "+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "daystream2.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---+--------------------+\n",
      "|line_num|   split(_c0, \t, -1)|pos|                 val|\n",
      "+--------+--------------------+---+--------------------+\n",
      "|       0|[_hc, 1611446400,...|  0|                 _hc|\n",
      "|       0|[_hc, 1611446400,...|  1|          1611446400|\n",
      "|       0|[_hc, 1611446400,...|  2|                 _hs|\n",
      "|       0|[_hc, 1611446400,...|  3|                   0|\n",
      "|       0|[_hc, 1611446400,...|  4|                  _c|\n",
      "|       0|[_hc, 1611446400,...|  5|          1611446400|\n",
      "|       0|[_hc, 1611446400,...|  6|                  _s|\n",
      "|       0|[_hc, 1611446400,...|  7|                   0|\n",
      "|       0|[_hc, 1611446400,...|  8|                type|\n",
      "|       0|[_hc, 1611446400,...|  9|            position|\n",
      "|       0|[_hc, 1611446400,...| 10|               ident|\n",
      "|       0|[_hc, 1611446400,...| 11|              AWQ320|\n",
      "|       0|[_hc, 1611446400,...| 12|             childID|\n",
      "|       0|[_hc, 1611446400,...| 13|                 197|\n",
      "|       0|[_hc, 1611446400,...| 14|                  _t|\n",
      "|       0|[_hc, 1611446400,...| 15|                   b|\n",
      "|       0|[_hc, 1611446400,...| 16|        adsb_version|\n",
      "|       0|[_hc, 1611446400,...| 17|                   0|\n",
      "|       0|[_hc, 1611446400,...| 18|       airGround_src|\n",
      "|       0|[_hc, 1611446400,...| 19|                   A|\n",
      "|       0|[_hc, 1611446400,...| 20|           airground|\n",
      "|       0|[_hc, 1611446400,...| 21|                   A|\n",
      "|       0|[_hc, 1611446400,...| 22|                 alt|\n",
      "|       0|[_hc, 1611446400,...| 23|                 367|\n",
      "|       0|[_hc, 1611446400,...| 24|              alt_ft|\n",
      "|       0|[_hc, 1611446400,...| 25|               36700|\n",
      "|       0|[_hc, 1611446400,...| 26|        alt_gnss_src|\n",
      "|       0|[_hc, 1611446400,...| 27|                   A|\n",
      "|       0|[_hc, 1611446400,...| 28|        alt_pressure|\n",
      "|       0|[_hc, 1611446400,...| 29|               36700|\n",
      "|       0|[_hc, 1611446400,...| 30|             alt_src|\n",
      "|       0|[_hc, 1611446400,...| 31|                   A|\n",
      "|       0|[_hc, 1611446400,...| 32|             bitmask|\n",
      "|       0|[_hc, 1611446400,...| 33|                   0|\n",
      "|       0|[_hc, 1611446400,...| 34|               clock|\n",
      "|       0|[_hc, 1611446400,...| 35|          1611446391|\n",
      "|       0|[_hc, 1611446400,...| 36|              combid|\n",
      "|       0|[_hc, 1611446400,...| 37|        1611446400-7|\n",
      "|       0|[_hc, 1611446400,...| 38|                dest|\n",
      "|       0|[_hc, 1611446400,...| 39|                WMKK|\n",
      "|       0|[_hc, 1611446400,...| 40|            facility|\n",
      "|       0|[_hc, 1611446400,...| 41|fAT-b68f52fd-d1f7...|\n",
      "|       0|[_hc, 1611446400,...| 42|                feed|\n",
      "|       0|[_hc, 1611446400,...| 43|             ADEPT22|\n",
      "|       0|[_hc, 1611446400,...| 44|              feed_c|\n",
      "|       0|[_hc, 1611446400,...| 45|          1611446399|\n",
      "|       0|[_hc, 1611446400,...| 46|              feed_s|\n",
      "|       0|[_hc, 1611446400,...| 47|                4180|\n",
      "|       0|[_hc, 1611446400,...| 48|         flightlevel|\n",
      "|       0|[_hc, 1611446400,...| 49|                 367|\n",
      "|       0|[_hc, 1611446400,...| 50|flightlevel_pressure|\n",
      "|       0|[_hc, 1611446400,...| 51|                 367|\n",
      "|       0|[_hc, 1611446400,...| 52|                  fp|\n",
      "|       0|[_hc, 1611446400,...| 53|AWQ320-1611267240...|\n",
      "|       0|[_hc, 1611446400,...| 54|             gSource|\n",
      "|       0|[_hc, 1611446400,...| 55|                feed|\n",
      "|       0|[_hc, 1611446400,...| 56|                  gs|\n",
      "|       0|[_hc, 1611446400,...| 57|                 501|\n",
      "|       0|[_hc, 1611446400,...| 58|              gs_src|\n",
      "|       0|[_hc, 1611446400,...| 59|                   A|\n",
      "|       0|[_hc, 1611446400,...| 60|             hSource|\n",
      "|       0|[_hc, 1611446400,...| 61|                feed|\n",
      "|       0|[_hc, 1611446400,...| 62|             heading|\n",
      "|       0|[_hc, 1611446400,...| 63|                 296|\n",
      "|       0|[_hc, 1611446400,...| 64|    heading_magnetic|\n",
      "|       0|[_hc, 1611446400,...| 65|               297.1|\n",
      "|       0|[_hc, 1611446400,...| 66|         heading_src|\n",
      "|       0|[_hc, 1611446400,...| 67|                   A|\n",
      "|       0|[_hc, 1611446400,...| 68|               hexid|\n",
      "|       0|[_hc, 1611446400,...| 69|              8A02CB|\n",
      "|       0|[_hc, 1611446400,...| 70|                 lat|\n",
      "|       0|[_hc, 1611446400,...| 71|             1.83881|\n",
      "|       0|[_hc, 1611446400,...| 72|                 lon|\n",
      "|       0|[_hc, 1611446400,...| 73|           103.30525|\n",
      "|       0|[_hc, 1611446400,...| 74|                mach|\n",
      "|       0|[_hc, 1611446400,...| 75|               0.760|\n",
      "|       0|[_hc, 1611446400,...| 76|               nac_p|\n",
      "|       0|[_hc, 1611446400,...| 77|                   7|\n",
      "|       0|[_hc, 1611446400,...| 78|               nac_v|\n",
      "|       0|[_hc, 1611446400,...| 79|                   0|\n",
      "|       0|[_hc, 1611446400,...| 80|             nav_alt|\n",
      "|       0|[_hc, 1611446400,...| 81|               30000|\n",
      "|       0|[_hc, 1611446400,...| 82|             nav_qnh|\n",
      "|       0|[_hc, 1611446400,...| 83|              1009.0|\n",
      "|       0|[_hc, 1611446400,...| 84|                orig|\n",
      "|       0|[_hc, 1611446400,...| 85|                WARR|\n",
      "|       0|[_hc, 1611446400,...| 86|      otherPedigrees|\n",
      "|       0|[_hc, 1611446400,...| 87|{aireon sita_ups ...|\n",
      "|       0|[_hc, 1611446400,...| 88|            pedigree|\n",
      "|       0|[_hc, 1611446400,...| 89|         aireon wide|\n",
      "|       0|[_hc, 1611446400,...| 90|             pos_nic|\n",
      "|       0|[_hc, 1611446400,...| 91|                   7|\n",
      "|       0|[_hc, 1611446400,...| 92|              pos_rc|\n",
      "|       0|[_hc, 1611446400,...| 93|                 371|\n",
      "|       0|[_hc, 1611446400,...| 94|        position_src|\n",
      "|       0|[_hc, 1611446400,...| 95|                   A|\n",
      "|       0|[_hc, 1611446400,...| 96|           preferred|\n",
      "|       0|[_hc, 1611446400,...| 97|                   1|\n",
      "|       0|[_hc, 1611446400,...| 98|            pressure|\n",
      "|       0|[_hc, 1611446400,...| 99|                 220|\n",
      "|       0|[_hc, 1611446400,...|100|          provenance|\n",
      "|       0|[_hc, 1611446400,...|101|                wide|\n",
      "|       0|[_hc, 1611446400,...|102|               recvd|\n",
      "|       0|[_hc, 1611446400,...|103|          1611446400|\n",
      "|       0|[_hc, 1611446400,...|104|                 reg|\n",
      "|       0|[_hc, 1611446400,...|105|               PKAXU|\n",
      "|       0|[_hc, 1611446400,...|106|                roll|\n",
      "|       0|[_hc, 1611446400,...|107|                 0.0|\n",
      "|       0|[_hc, 1611446400,...|108|               score|\n",
      "|       0|[_hc, 1611446400,...|109|                1000|\n",
      "|       0|[_hc, 1611446400,...|110|                 sil|\n",
      "|       0|[_hc, 1611446400,...|111|                   2|\n",
      "|       0|[_hc, 1611446400,...|112|            sil_type|\n",
      "|       0|[_hc, 1611446400,...|113|             unknown|\n",
      "|       0|[_hc, 1611446400,...|114|           speed_ias|\n",
      "|       0|[_hc, 1611446400,...|115|                 248|\n",
      "|       0|[_hc, 1611446400,...|116|           speed_tas|\n",
      "|       0|[_hc, 1611446400,...|117|                 442|\n",
      "|       0|[_hc, 1611446400,...|118|              squawk|\n",
      "|       0|[_hc, 1611446400,...|119|                7004|\n",
      "|       0|[_hc, 1611446400,...|120|          squawk_src|\n",
      "|       0|[_hc, 1611446400,...|121|                   U|\n",
      "|       0|[_hc, 1611446400,...|122|         temperature|\n",
      "|       0|[_hc, 1611446400,...|123|                 -50|\n",
      "|       0|[_hc, 1611446400,...|124| temperature_quality|\n",
      "|       0|[_hc, 1611446400,...|125|                   1|\n",
      "|       0|[_hc, 1611446400,...|126|          track_rate|\n",
      "|       0|[_hc, 1611446400,...|127|               -0.03|\n",
      "|       0|[_hc, 1611446400,...|128|          updateType|\n",
      "|       0|[_hc, 1611446400,...|129|                   A|\n",
      "|       0|[_hc, 1611446400,...|130|            vertRate|\n",
      "|       0|[_hc, 1611446400,...|131|               -2592|\n",
      "|       0|[_hc, 1611446400,...|132|       vertRate_geom|\n",
      "|       0|[_hc, 1611446400,...|133|               -2752|\n",
      "|       0|[_hc, 1611446400,...|134|   vertRate_geom_src|\n",
      "|       0|[_hc, 1611446400,...|135|                   A|\n",
      "|       0|[_hc, 1611446400,...|136|        vertRate_src|\n",
      "|       0|[_hc, 1611446400,...|137|                   U|\n",
      "|       0|[_hc, 1611446400,...|138|            wind_dir|\n",
      "|       0|[_hc, 1611446400,...|139|                 107|\n",
      "|       0|[_hc, 1611446400,...|140|        wind_quality|\n",
      "|       0|[_hc, 1611446400,...|141|                   1|\n",
      "|       0|[_hc, 1611446400,...|142|          wind_speed|\n",
      "|       0|[_hc, 1611446400,...|143|                  59|\n",
      "|       1|[_hc, 1611446400,...|  0|                 _hc|\n",
      "|       1|[_hc, 1611446400,...|  1|          1611446400|\n",
      "|       1|[_hc, 1611446400,...|  2|                 _hs|\n",
      "|       1|[_hc, 1611446400,...|  3|                   1|\n",
      "|       1|[_hc, 1611446400,...|  4|                  _c|\n",
      "|       1|[_hc, 1611446400,...|  5|          1611446400|\n",
      "|       1|[_hc, 1611446400,...|  6|                  _s|\n",
      "|       1|[_hc, 1611446400,...|  7|                   1|\n",
      "|       1|[_hc, 1611446400,...|  8|                type|\n",
      "|       1|[_hc, 1611446400,...|  9|            position|\n",
      "|       1|[_hc, 1611446400,...| 10|               ident|\n",
      "|       1|[_hc, 1611446400,...| 11|              AWQ320|\n",
      "|       1|[_hc, 1611446400,...| 12|             childID|\n",
      "|       1|[_hc, 1611446400,...| 13|                 197|\n",
      "|       1|[_hc, 1611446400,...| 14|                  _t|\n",
      "|       1|[_hc, 1611446400,...| 15|                   b|\n",
      "|       1|[_hc, 1611446400,...| 16|        adsb_version|\n",
      "|       1|[_hc, 1611446400,...| 17|                   0|\n",
      "|       1|[_hc, 1611446400,...| 18|       airGround_src|\n",
      "|       1|[_hc, 1611446400,...| 19|                   A|\n",
      "|       1|[_hc, 1611446400,...| 20|           airground|\n",
      "|       1|[_hc, 1611446400,...| 21|                   A|\n",
      "|       1|[_hc, 1611446400,...| 22|                 alt|\n",
      "|       1|[_hc, 1611446400,...| 23|                 367|\n",
      "|       1|[_hc, 1611446400,...| 24|              alt_ft|\n",
      "|       1|[_hc, 1611446400,...| 25|               36700|\n",
      "|       1|[_hc, 1611446400,...| 26|        alt_gnss_src|\n",
      "|       1|[_hc, 1611446400,...| 27|                   A|\n",
      "|       1|[_hc, 1611446400,...| 28|        alt_pressure|\n",
      "|       1|[_hc, 1611446400,...| 29|               36700|\n",
      "|       1|[_hc, 1611446400,...| 30|             alt_src|\n",
      "|       1|[_hc, 1611446400,...| 31|                   A|\n",
      "|       1|[_hc, 1611446400,...| 32|             bitmask|\n",
      "|       1|[_hc, 1611446400,...| 33|                   0|\n",
      "|       1|[_hc, 1611446400,...| 34|               clock|\n",
      "|       1|[_hc, 1611446400,...| 35|          1611446391|\n",
      "|       1|[_hc, 1611446400,...| 36|              combid|\n",
      "|       1|[_hc, 1611446400,...| 37|        1611446400-7|\n",
      "|       1|[_hc, 1611446400,...| 38|                dest|\n",
      "|       1|[_hc, 1611446400,...| 39|                WMKK|\n",
      "|       1|[_hc, 1611446400,...| 40|            facility|\n",
      "|       1|[_hc, 1611446400,...| 41|fAT-b68f52fd-d1f7...|\n",
      "|       1|[_hc, 1611446400,...| 42|                feed|\n",
      "|       1|[_hc, 1611446400,...| 43|             ADEPT22|\n",
      "|       1|[_hc, 1611446400,...| 44|              feed_c|\n",
      "|       1|[_hc, 1611446400,...| 45|          1611446399|\n",
      "|       1|[_hc, 1611446400,...| 46|              feed_s|\n",
      "|       1|[_hc, 1611446400,...| 47|                4180|\n",
      "|       1|[_hc, 1611446400,...| 48|         flightlevel|\n",
      "|       1|[_hc, 1611446400,...| 49|                 367|\n",
      "|       1|[_hc, 1611446400,...| 50|flightlevel_pressure|\n",
      "|       1|[_hc, 1611446400,...| 51|                 367|\n",
      "|       1|[_hc, 1611446400,...| 52|                  fp|\n",
      "|       1|[_hc, 1611446400,...| 53|AWQ320-1611267240...|\n",
      "|       1|[_hc, 1611446400,...| 54|             gSource|\n",
      "|       1|[_hc, 1611446400,...| 55|                feed|\n",
      "+--------+--------------------+---+--------------------+\n",
      "only showing top 200 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "daystream2.select(\n",
    "    \"line_num\",\n",
    "    F.split(\"_c0\", '\\t'),\n",
    "    F.posexplode(F.split(\"_c0\", '\\t')).alias(\"pos\", \"val\")\n",
    ")\\\n",
    ".show(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o219.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 14.0 failed 1 times, most recent failure: Lost task 7.0 in stage 14.0 (TID 693, 192.168.146.183, executor driver): java.lang.OutOfMemoryError: Java heap space\n\tat java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)\n\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:335)\n\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:105)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:74)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:473)\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:172)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$2(SortAggregateExec.scala:80)\n\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$2$adapted(SortAggregateExec.scala:77)\n\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec$$Lambda$3507/1150688301.apply(Unknown Source)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:859)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:859)\n\tat org.apache.spark.rdd.RDD$$Lambda$2366/1594000826.apply(Unknown Source)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2004/644433456.apply(Unknown Source)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2120)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2139)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:467)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:420)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3627)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2697)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3618)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3616)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2697)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2904)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:300)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:337)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)\n\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:335)\n\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:105)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:74)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:473)\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:172)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$2(SortAggregateExec.scala:80)\n\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$2$adapted(SortAggregateExec.scala:77)\n\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec$$Lambda$3507/1150688301.apply(Unknown Source)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:859)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:859)\n\tat org.apache.spark.rdd.RDD$$Lambda$2366/1594000826.apply(Unknown Source)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2004/644433456.apply(Unknown Source)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-abbf5d49be8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m )\\\n\u001b[1;32m     12\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"line_num\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpivot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"val\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/src/data-engineering/flights-analytics/.venv/lib/python3.7/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    438\u001b[0m         \"\"\"\n\u001b[1;32m    439\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/data-engineering/flights-analytics/.venv/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/data-engineering/flights-analytics/.venv/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/data-engineering/flights-analytics/.venv/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o219.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 14.0 failed 1 times, most recent failure: Lost task 7.0 in stage 14.0 (TID 693, 192.168.146.183, executor driver): java.lang.OutOfMemoryError: Java heap space\n\tat java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)\n\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:335)\n\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:105)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:74)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:473)\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:172)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$2(SortAggregateExec.scala:80)\n\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$2$adapted(SortAggregateExec.scala:77)\n\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec$$Lambda$3507/1150688301.apply(Unknown Source)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:859)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:859)\n\tat org.apache.spark.rdd.RDD$$Lambda$2366/1594000826.apply(Unknown Source)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2004/644433456.apply(Unknown Source)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2120)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2139)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:467)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:420)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3627)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2697)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3618)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3616)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2697)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2904)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:300)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:337)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)\n\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:335)\n\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:105)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:74)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:473)\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:172)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$2(SortAggregateExec.scala:80)\n\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$2$adapted(SortAggregateExec.scala:77)\n\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec$$Lambda$3507/1150688301.apply(Unknown Source)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:859)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:859)\n\tat org.apache.spark.rdd.RDD$$Lambda$2366/1594000826.apply(Unknown Source)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2004/644433456.apply(Unknown Source)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n"
     ]
    }
   ],
   "source": [
    "daystream2.select(\n",
    "    \"line_num\",\n",
    "    F.split(\"_c0\", '\\t').alias(\"line\"),\n",
    "    F.posexplode(F.split(\"_c0\", '\\t')).alias(\"pos\", \"val\")\n",
    ")\\\n",
    ".drop(\"val\")\\\n",
    ".select(\n",
    "    \"line_num\",\n",
    "    F.concat(F.lit(\"col\"),F.col(\"pos\").cast(\"string\")).alias(\"name\"),\n",
    "    F.expr(\"line[pos]\").alias(\"val\")\n",
    ")\\\n",
    ".groupBy(\"line_num\").pivot(\"name\").agg(F.first(\"val\"))\\\n",
    ".show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
