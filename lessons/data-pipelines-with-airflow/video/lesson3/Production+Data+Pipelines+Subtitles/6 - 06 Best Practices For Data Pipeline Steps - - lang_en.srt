1
00:00:00,000 --> 00:00:02,580
Once you start to build your own DAGs in Airflow,

2
00:00:02,580 --> 00:00:06,810
you may begin to wonder what the right boundaries are between tasks.

3
00:00:06,810 --> 00:00:08,430
In our bike share example,

4
00:00:08,430 --> 00:00:11,565
tasks within the DAG have been defined for you.

5
00:00:11,565 --> 00:00:16,845
Have you stopped to wonder why we have two S3 to Redshift load operations?

6
00:00:16,845 --> 00:00:20,055
Why do we have two data quality checks instead of just one?

7
00:00:20,055 --> 00:00:21,540
Why not just do them together?

8
00:00:21,540 --> 00:00:25,185
Though it may seem obvious with the DAG sketched out in front of you,

9
00:00:25,185 --> 00:00:28,860
in practice, finding these boundaries can be harder than you might think.

10
00:00:28,860 --> 00:00:31,230
In this section, we'll cover some good rules to

11
00:00:31,230 --> 00:00:34,710
use when reasoning about how to break apart tasks.

12
00:00:34,710 --> 00:00:38,525
Every task in your DAG should perform only one job.

13
00:00:38,525 --> 00:00:41,460
To paraphrase Ken Thompson's Unix philosophy,

14
00:00:41,460 --> 00:00:45,110
write programs that do one thing, and do it well.

15
00:00:45,110 --> 00:00:48,320
We can apply the same concept in writing the task within

16
00:00:48,320 --> 00:00:53,120
our DAGs: Write tasks that do one thing, and do it well.

17
00:00:53,120 --> 00:00:57,170
This is useful for you as a developer as you build your data pipelines.

18
00:00:57,170 --> 00:01:00,920
If you revisit a pipeline you wrote six months ago,

19
00:01:00,920 --> 00:01:05,360
you'll have a much easier time understanding how it works in

20
00:01:05,360 --> 00:01:10,220
the lineage of the data if the boundaries between tasks are clear and well defined.

21
00:01:10,220 --> 00:01:14,435
This is true in the code itself and with an Airflow UI.

22
00:01:14,435 --> 00:01:16,010
So, you can see here,

23
00:01:16,010 --> 00:01:18,035
we have just a single task.

24
00:01:18,035 --> 00:01:22,675
It's pretty descriptive as to what it's doing: copying data from S3 to Redshift,

25
00:01:22,675 --> 00:01:24,650
and it gives us a pretty clear idea of

26
00:01:24,650 --> 00:01:27,380
exactly the operations that are happening within the task.

27
00:01:27,380 --> 00:01:29,510
If we did three or four other things here,

28
00:01:29,510 --> 00:01:32,810
this wouldn't make as much sense and it wouldn't be obvious what it was doing.

29
00:01:32,810 --> 00:01:37,325
Tasks that do just one thing are often more easily parallelized.

30
00:01:37,325 --> 00:01:42,860
This parallelization can offer a significant speedup in the execution of our DAGs.

31
00:01:42,860 --> 00:01:46,135
We've already observed this effect in our bike share DAG.

32
00:01:46,135 --> 00:01:49,910
You can see these two tasks are running at the same time.

33
00:01:49,910 --> 00:01:52,700
You may have noticed that in the final exercise of Lesson

34
00:01:52,700 --> 00:01:57,070
two that multiple copy and data quality checks could run at once.

35
00:01:57,070 --> 00:02:01,250
This is because there are not cross-dependencies between these data sets.

36
00:02:01,250 --> 00:02:03,455
If we had additional data sets,

37
00:02:03,455 --> 00:02:06,470
we could add more copies and data quality checks.

38
00:02:06,470 --> 00:02:10,145
In Airflow, we parallelize those copies and data quality checks as well.

39
00:02:10,145 --> 00:02:12,830
One other thing I just want to emphasize here in this diagram,

40
00:02:12,830 --> 00:02:17,675
so if this is our load trips and load stations task, when properly parallelized,

41
00:02:17,675 --> 00:02:19,460
if we have time equals zero,

42
00:02:19,460 --> 00:02:20,945
so this is zero seconds,

43
00:02:20,945 --> 00:02:22,520
this is six seconds,

44
00:02:22,520 --> 00:02:24,310
and this is 10 seconds,

45
00:02:24,310 --> 00:02:27,365
Airflow will run both of these things at the same time.

46
00:02:27,365 --> 00:02:31,940
So, the total duration will be whatever the longest running task is.

47
00:02:31,940 --> 00:02:33,200
So, in this case,

48
00:02:33,200 --> 00:02:35,165
load stations took six seconds,

49
00:02:35,165 --> 00:02:36,830
load trips took 10 seconds.

50
00:02:36,830 --> 00:02:41,040
So, the total execution for both of them will be about 10 seconds.

51
00:02:41,040 --> 00:02:45,100
If instead we bundle all of our data loading and quality checks into just a few tasks,

52
00:02:45,100 --> 00:02:47,645
it could take much longer to run our DAG.

53
00:02:47,645 --> 00:02:52,915
So, if for whatever reason we decided to put these two tasks together into a single task,

54
00:02:52,915 --> 00:02:55,220
then we would get the worst of both worlds.

55
00:02:55,220 --> 00:02:57,395
Not only would we lose visibility,

56
00:02:57,395 --> 00:02:59,930
we would also see that the execution time would go up.

57
00:02:59,930 --> 00:03:04,040
So, again, if this is zero seconds and this is 16 seconds,

58
00:03:04,040 --> 00:03:07,255
we know from our previous slide that load trips takes about 10 seconds.

59
00:03:07,255 --> 00:03:12,740
If I had load stations running the same task after that and it took six seconds,

60
00:03:12,740 --> 00:03:16,100
we would have a total of 16 seconds of execution time.

61
00:03:16,100 --> 00:03:17,735
When a task fails,

62
00:03:17,735 --> 00:03:21,250
it's important to quickly identify the issue and resolve the problem.

63
00:03:21,250 --> 00:03:24,165
When each task in your DAG performs a single purpose,

64
00:03:24,165 --> 00:03:29,045
it's easy to look at the Airflow UI to determine what exactly failed.

65
00:03:29,045 --> 00:03:32,330
However, if your tasks are performing more than one task each,

66
00:03:32,330 --> 00:03:34,715
you'll often struggle to figure out what broke.

67
00:03:34,715 --> 00:03:37,790
Again, if in our bike share example we had loaded

68
00:03:37,790 --> 00:03:41,630
both the trips and stations data at the same time in a single task,

69
00:03:41,630 --> 00:03:45,395
how do we know which data set had failed to load if that task failed?

70
00:03:45,395 --> 00:03:47,030
Was it the trips data set?

71
00:03:47,030 --> 00:03:48,740
Was it the stations data set?

72
00:03:48,740 --> 00:03:52,100
Maybe both of them failed to load. It's not clear.

73
00:03:52,100 --> 00:03:56,600
Well-defined boundaries make our tasks more maintainable and easy to debug.

74
00:03:56,600 --> 00:03:59,360
You can see in this example here that

75
00:03:59,360 --> 00:04:02,810
Airflow uses the dark red to indicate that a task has failed.

76
00:04:02,810 --> 00:04:05,910
The dark yellow means that an upstream task that

77
00:04:05,910 --> 00:04:09,735
this task depends on has failed and this task can no longer run.

78
00:04:09,735 --> 00:04:14,540
So, load trips from S3 to Redshift depends on the create trips table.

79
00:04:14,540 --> 00:04:16,640
So, if the create trips table has failed,

80
00:04:16,640 --> 00:04:18,750
then we can't run this task.

