1
00:00:00,000 --> 00:00:04,035
Data lineage describes the discrete steps involved in the movement,

2
00:00:04,035 --> 00:00:07,335
creation, and calculation of a data set.

3
00:00:07,335 --> 00:00:09,735
For example, in Lesson one,

4
00:00:09,735 --> 00:00:12,810
we created a station traffic table which

5
00:00:12,810 --> 00:00:17,070
contained a lifetime count of arrivals and departures at every bike share station.

6
00:00:17,070 --> 00:00:20,565
We would be doing that in this redshift analysis step.

7
00:00:20,565 --> 00:00:23,595
The diagram on the slide should look familiar.

8
00:00:23,595 --> 00:00:26,940
We have a task for S3 to redshift,

9
00:00:26,940 --> 00:00:30,455
and we have another task for redshift analysis.

10
00:00:30,455 --> 00:00:34,550
The output of those two tests is the table below,

11
00:00:34,550 --> 00:00:36,680
where we count the number of departures

12
00:00:36,680 --> 00:00:40,265
and the number of arrivals for each of our bike stations.

13
00:00:40,265 --> 00:00:43,290
So, you can see here we have station ID,

14
00:00:43,290 --> 00:00:45,030
which is this bike station,

15
00:00:45,030 --> 00:00:47,390
we have number of departures from that station,

16
00:00:47,390 --> 00:00:50,270
as well as the number of arrivals from that station.

17
00:00:50,270 --> 00:00:53,560
At some point, other engineers,

18
00:00:53,560 --> 00:00:57,545
analysts, or other data consumers willing to make use of this data.

19
00:00:57,545 --> 00:01:00,260
When these data consumers ask how the data was

20
00:01:00,260 --> 00:01:03,590
calculated and what the source of the data for the calculation was,

21
00:01:03,590 --> 00:01:05,500
we'll need to be able to tell them.

22
00:01:05,500 --> 00:01:07,805
Being able to describe the data lineage

23
00:01:07,805 --> 00:01:10,160
of our station traffic table will build confidence

24
00:01:10,160 --> 00:01:12,410
in our users that are data pipeline is creating

25
00:01:12,410 --> 00:01:15,380
meaningful results using the correct data sets.

26
00:01:15,380 --> 00:01:19,190
If we can't answer questions and we're not sure what our data lineages,

27
00:01:19,190 --> 00:01:24,050
it's less likely that our data consumers will trust our data sets and choose to use them.

28
00:01:24,050 --> 00:01:27,800
So, again, if someone in our organization asked us how we calculated

29
00:01:27,800 --> 00:01:32,255
the number of departures or the number of arrivals for say station 192,

30
00:01:32,255 --> 00:01:34,820
we could actually point to this diagram up here and tell them

31
00:01:34,820 --> 00:01:37,460
exactly what happened, step-by-step.

32
00:01:37,460 --> 00:01:41,830
Here, we took data from the overall trips for all 2018,

33
00:01:41,830 --> 00:01:44,430
and then we copy that CVS to redshift,

34
00:01:44,430 --> 00:01:45,640
and then within redshift,

35
00:01:45,640 --> 00:01:48,400
we performed analysis on that data set.

36
00:01:48,400 --> 00:01:50,500
Another major benefit of servicing

37
00:01:50,500 --> 00:01:53,560
the data lineage of your data sets is that it allows everyone in

38
00:01:53,560 --> 00:01:58,450
the organization to agree on the definition of how a particular metric is calculated.

39
00:01:58,450 --> 00:02:00,625
The more our bikeshare company grows,

40
00:02:00,625 --> 00:02:05,200
the more people and divisions there will be that need to agree on key metrics.

41
00:02:05,200 --> 00:02:08,470
If one person or group has one definition of a

42
00:02:08,470 --> 00:02:11,890
metric and another person or group has a different definition,

43
00:02:11,890 --> 00:02:14,420
we're going to be in a lot of trouble as a business.

44
00:02:14,420 --> 00:02:16,810
Describing and servicing data lineage is one

45
00:02:16,810 --> 00:02:19,120
of the key ways we can ensure that everyone in

46
00:02:19,120 --> 00:02:21,040
the organization has access to and

47
00:02:21,040 --> 00:02:24,310
understands where data originates and how it's calculated.

48
00:02:24,310 --> 00:02:27,545
So, again, coming back to this table,

49
00:02:27,545 --> 00:02:29,055
we are data engineers,

50
00:02:29,055 --> 00:02:31,320
we designed our tasks.

51
00:02:31,320 --> 00:02:34,910
If we have a business analyst or a data scientist who are partnered with,

52
00:02:34,910 --> 00:02:36,500
who are our data customers,

53
00:02:36,500 --> 00:02:39,850
they might ask us to prove that this number is correct.

54
00:02:39,850 --> 00:02:41,120
In this particular case,

55
00:02:41,120 --> 00:02:42,950
the calculation is not very involved,

56
00:02:42,950 --> 00:02:47,600
but it's still important for us to be able to tell our data consumers exactly where

57
00:02:47,600 --> 00:02:49,820
our data came from and how we calculated

58
00:02:49,820 --> 00:02:52,805
it so that they trust that those numbers are correct.

59
00:02:52,805 --> 00:02:55,424
Finally, as a data engineer,

60
00:02:55,424 --> 00:02:59,225
data lineage helps us track down the route of errors when they occur.

61
00:02:59,225 --> 00:03:03,125
If each step of the data movement and transformation process is well described,

62
00:03:03,125 --> 00:03:05,210
we can easily find problems when they occur.

63
00:03:05,210 --> 00:03:07,370
So, let's pretend again as

64
00:03:07,370 --> 00:03:11,615
data engineers that we had a problem running this redshift analysis,

65
00:03:11,615 --> 00:03:15,810
and it's entirely possible that in the last exercise while you're doing it,

66
00:03:15,810 --> 00:03:19,835
you may have had an issue completing the lesson successfully the first time.

67
00:03:19,835 --> 00:03:23,030
It's likely that maybe the table wasn't produced at all,

68
00:03:23,030 --> 00:03:26,090
or maybe the table is produced with the wrong information.

69
00:03:26,090 --> 00:03:30,140
In that scenario, we want to be able to know that the analysis failed.

70
00:03:30,140 --> 00:03:31,760
In this particular case,

71
00:03:31,760 --> 00:03:34,940
we might have not a number or NaNs in all of our rows,

72
00:03:34,940 --> 00:03:38,030
and so this would be an indicator that something has failed.

73
00:03:38,030 --> 00:03:41,435
There are additional steps we can add to our pipeline

74
00:03:41,435 --> 00:03:45,815
to actually verify that we've run into these types of issues.

75
00:03:45,815 --> 00:03:48,170
We're going to talk about that a little bit later in this lesson.

76
00:03:48,170 --> 00:03:51,260
But for now, just seeing that the analysis step has failed

77
00:03:51,260 --> 00:03:55,560
is already a really good start for us to find issues in our pipelines.

78
00:03:55,870 --> 00:03:59,030
Returning to the last exercise of lesson one,

79
00:03:59,030 --> 00:04:01,555
if our location traffic task had failed,

80
00:04:01,555 --> 00:04:04,010
we can immediately identify the location of

81
00:04:04,010 --> 00:04:06,665
that failure by using the graph view and airflow.

82
00:04:06,665 --> 00:04:08,435
As you can see in this diagram,

83
00:04:08,435 --> 00:04:11,720
location traffic, a circle with red to show that it failed.

84
00:04:11,720 --> 00:04:14,895
Again, the location traffic failed here,

85
00:04:14,895 --> 00:04:18,730
and some of you very well may have seen something similar in your first exercise.

